{"componentChunkName":"component---src-templates-blog-post-js","path":"/tutorial-pytorch-speechcommands/output/","result":{"data":{"markdownRemark":{"html":"<p>This is a tutorial post on speech commands recognition using the <a href=\"https://arxiv.org/abs/1804.03209\">Speech Commands dataset</a>. The goals for this post</p>\n<ol>\n<li>Work with audio data using torchaudio: look at spectrograms features and data augmentation</li>\n<li>Train a model to recogize audio data from a vocabulary of spoken commands</li>\n<li>Evaluate model performance using measures like accuracy (error rate) and confusion matrix</li>\n</ol>\n<p>There are around 10 speech commands like Yes, No, Up, Down and so on. The dataset contains wav files sampled at 16000 Hz and each command waveform contains upto a second of data. In addition to the commands themselves, there are some words that are not commands (and phonetically diverse) and background files containing background noise and silence segments.</p>\n<p>We will rely heavily on the torchaudio package for input preprocessing. It has some convenient dataloaders and feature preprocessing transforms. The nice thing about torchaudio, is that the feature processing can take place on the GPU and this will accelerate the training process significantly.</p>\n<p>To install torchaudio, the command below should be executed in a terminal:<br>\nconda install -c pytorch torchaudio</p>\n<p>The -c option searches on the pytorch channel.</p>\n<p>This notebook is available on github at this <a href=\"https://github.com/jumpml/pytorch-tutorials/blob/master/SpeechCommands_CNN.ipynb\">link</a></p>\n<h2>Data Setup</h2>\n<p>We have created a wrapper around torchaudio’s SPEECHCOMMANDS dataset class. The wrapper performs the following steps for us</p>\n<ol>\n<li>Create train, val and test splits of the dataset</li>\n<li>Process the background noise files and creates 1 sec segments for augmenting each split</li>\n<li>Pad the data to 1s and map the string labels to integer. Number 11 corresponds to background or unknown.</li>\n<li>Creates dataloaders for each split with appropriate transforms (MelSpectrogram, etc.)</li>\n</ol>\n<p>The torchaudio class automatically downloads the data if not already on disk. If we look at the source code for the speechcommands dataset class in torchaudio, we need to specify the version of speech commands (v0.2). The dataset is around 2.3GB in size. Each item in the dataloader consists of the following information:<br>\nwaveform, sample<em>rate, label, speaker</em>id, utterance_number.</p>\n<p>In this tutorial, we are not using speaker<em>id or utterance</em>number. We are not downsampling the audio to 8kHz, which may be a good thing to do, since it is an effective way to throw away information that is not related to the task of recognizing commands. Most of the voice information is between 20 Hz to 3.6 kHz.</p>\n<h3>Feature Pre-Processing</h3>\n<p>Torchaudio comes with several useful <a href=\"https://pytorch.org/audio/transforms.html\">transforms</a>, which we can use to get our input features efficiently computed on GPU.</p>\n<p>Here is the feature processing we use for training data</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">self<span class=\"token punctuation\">.</span>train_transform <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>\n   torchaudio<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>MelSpectrogram<span class=\"token punctuation\">(</span>sample_rate<span class=\"token operator\">=</span><span class=\"token number\">16000</span><span class=\"token punctuation\">,</span> n_fft<span class=\"token operator\">=</span><span class=\"token number\">320</span><span class=\"token punctuation\">,</span> hop_length<span class=\"token operator\">=</span><span class=\"token number\">160</span><span class=\"token punctuation\">,</span> n_mels<span class=\"token operator\">=</span>n_mels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n   torchaudio<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>FrequencyMasking<span class=\"token punctuation\">(</span>freq_mask_param<span class=\"token operator\">=</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>n_mels<span class=\"token operator\">*</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n   torchaudio<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>TimeMasking<span class=\"token punctuation\">(</span>time_mask_param<span class=\"token operator\">=</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span><span class=\"token number\">0.2</span> <span class=\"token operator\">*</span> <span class=\"token number\">16000</span><span class=\"token operator\">/</span><span class=\"token number\">160</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n   torchaudio<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>AmplitudeToDB<span class=\"token punctuation\">(</span>stype<span class=\"token operator\">=</span><span class=\"token string\">'power'</span><span class=\"token punctuation\">,</span> top_db<span class=\"token operator\">=</span><span class=\"token number\">80</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>A spectrogram is used to map the a waveform to a time-frequency representation which is almost always used in any kind of speech processing. We use a hop length of 10ms and a FFT size of 20 ms. In order to keep our inputs small, we also employ mel filterbanks to get 64 features in total. Then we convert the spectrogram magnitude to log-power scale in dB with a minimum floor of -80 dB. In order to get more general features, we additionally apply Frequency and Time masking on the training set only.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">batch_size_train <span class=\"token operator\">=</span> <span class=\"token number\">64</span>\n<span class=\"token comment\"># Setup train, val and test dataloaders</span>\nsc_data <span class=\"token operator\">=</span> scd<span class=\"token punctuation\">.</span>SpeechCommandsData<span class=\"token punctuation\">(</span>train_bs<span class=\"token operator\">=</span>batch_size_train<span class=\"token punctuation\">,</span> test_bs<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span> val_bs<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span> n_mels<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/842d14906ffb1a2c699e465c17560cca/d74fe/output_5_0.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 94.5945945945946%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAxNAAAMTQHSzq1OAAAFWUlEQVR42i1UeUwUdxT+ZgaWw0UOa2ChViOkwVoFikZRW4zBWmtRIW2JWPEACVoFFawsLMslcshRPBCItRWkUI+gAiIeoSkWVjyQW5CKyGHwQmUR2J2d1x+0f7zM/N7vfd+843sDcC3h4B9mA81pcHt6BF88zYNX33H5zr503rUvHahPh8kdB2CNBRB9GFBlAJGH4X3lqLFdfJ5fy8GfnVvb0qC4kcVhnxqQdfdAeEQQOolfOUwWSa9IHvqS7KIek2z1SwK6CFNaFzBCBRBL/5mKTHf8TcYzj1NJoxep3iSTuVcl8ytHAZP2+xAa2aF9zDz+rc599Jbepa5W90F6z7hpwHM90KbjrOpcgfW2jGyMmcgIx00XFupgeUSvatil+67+3DhkFcQjogdwH2jEjEHCnFdi2JNUKZHCJauLA5JR1FuDUdAQ+1CzBKsmN5YhI1QzMjXzxRiATAl8iuR7OUfa8vJXA+9SxfxR/eC/GW2G0zg57qinwRpr6imyI59bv9P88tukyHnCgtpIMKpxAx9gBySycwKzeILZRTKfmkJNznLSFXHkXZTH/AXPIHw7chsrSVwdUj5MadBSNbT6OzKtn7ZAa1M2MIKVb7TC3Lsu8GywhX3FOyD7PbgSLeRXtCbGJ7X3PGdoaTOGi6o3jGN+cxc4z3NBmFWutvc4Erl904pYn9A1h3y3rUswXVGrhGtvFL+6TYnd/azczVNgnBYJHIhipSk5IT5B/nXmobW/ZcUHZv4Y6blZreIsM3YD+KUJXBlL9yLryx/sWcjsLHHyJ5LgoGXvLwguIuvhYlZyjAgujvmiJUy/QH5laeTTyzD7+g0wPc/8cf2AcUUTTOsI3J8GHsXEG1cRz5ezS60EBZFs7xBZlb1hhMvtwMUaJgg5KCVMK6X9JSGUT1vI5uhjA+bUMsyZAZZhRSNwmWB534DpjRJQM2nLgs9LqksJtGH0JNlQHyP0tgMfNzllAVEs5pSUfepLqYucJB8qNDhcf0oW/t0sQ/OWBlh0ED4a1EHxjJXUJgpzO0TNjQV62gFDaYOXuIZKmQ5DbDk+ScemLAlI0ENWLMaGB4hSPMSH92fqKmg5ZVJwL/DD6D9ciIHgrSNuIevZZ2M0dcNrurd+Fg06cjQcaEyUB3egWMFPlCswybAsrfdepfSze6g92I5uVH5Oh/pDKag+lakgrHs3FziYioAXB7GkLQNujZlWbjeyzoe5JV+NW5RUGr0k6Z6/PVu7ODnHhR8EfkoCdh3irHIy7VKvZc4uvpkl39RwEAtvJsOxMhImn5RcYEEdQDjrZUQbsK8d2MsstxlIYaZuAvY7m+H5NLi8b+S/H23h9w83W+Q8bofvcDscLrDYUIZVtrPVuwn7rbWdDEzgWSlc4uQWyKzzyDG4kqx9bpJgfIzMhAB3WJECG/XEJ+qJU4k0r+UuLXlURxbeEzJTEowm8OohLEj66wFkVUwKyXqBZ7JAsiE4JdxAr83EY31hEma3Mt9OV/SS7YedHfrFw9VkP9wlFo75GMppleHToAmMWuSEyR3vw7Lc6w8gXGNSiNbxUImQ5YslQR7iYL6l+DrbwhB9IkJ023Pa1Ymu2y4dq9Ipx2KkY+3bxe679mLaSIRo5F/CpHRAD2FC8DG9WJd7phMe3eyfVzy5JX7+ifTuK5AuBhPTpZrWRTT7fb27ouiaYjFV05n6tURL2Z0HqPCOH8kDCxgukv4nHEJ29ZairZfyGjam5tatzijUNJ5w1pASmlen5ZqRo9zte0VOmtQrAR9jY4cNn/ms1jcjv74ue57mQcxszYmqbRqTVQUaNtQ68DF32Y6X/wsE3shC83m7ugAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/842d14906ffb1a2c699e465c17560cca/fcda8/output_5_0.png\"\n        srcset=\"/static/842d14906ffb1a2c699e465c17560cca/12f09/output_5_0.png 148w,\n/static/842d14906ffb1a2c699e465c17560cca/e4a3f/output_5_0.png 295w,\n/static/842d14906ffb1a2c699e465c17560cca/fcda8/output_5_0.png 590w,\n/static/842d14906ffb1a2c699e465c17560cca/efc66/output_5_0.png 885w,\n/static/842d14906ffb1a2c699e465c17560cca/d74fe/output_5_0.png 1164w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Model Specification</h2>\n<p>The model we use is not too complicated:<br>\nInput X (B,1,64,101)<br>\n—> 32 Conv2D(8,20) —> BatchNorm2D —> MaxPool2D(2) —> ReLU<br>\n—> 8 Conv2D(4,10) —> BatchNorm2D —> MaxPool2D(2) —> ReLU<br>\n—> Linear(128) —> BatchNorm1D —> ReLU<br>\n—> Linear(11) —> LogSoftmax</p>\n<p>where B is Batch Size, the arguments to Conv2D are input numChannels, output numChannels and kernel size.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">model</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>conv1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span>in_channels<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> out_channels<span class=\"token operator\">=</span>num_filters_conv1<span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span>C1_kernel_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>bn1   <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>BatchNorm2d<span class=\"token punctuation\">(</span>num_filters_conv1<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>conv2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span>num_filters_conv1<span class=\"token punctuation\">,</span> num_filters_conv2<span class=\"token punctuation\">,</span> C2_kernel_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>bn2   <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>BatchNorm2d<span class=\"token punctuation\">(</span>num_filters_conv2<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>fc1_in_size<span class=\"token punctuation\">,</span> fc1_out_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>bn3   <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>BatchNorm1d<span class=\"token punctuation\">(</span>fc1_out_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>fc1_out_size<span class=\"token punctuation\">,</span> fc2_out_size<span class=\"token punctuation\">)</span>           <span class=\"token comment\"># number of classes = 11</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>F<span class=\"token punctuation\">.</span>max_pool2d<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>bn1<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>conv1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> mp2d_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>F<span class=\"token punctuation\">.</span>max_pool2d<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>bn2<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>conv2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> mp2d_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> fc1_in_size<span class=\"token punctuation\">)</span>    <span class=\"token comment\"># reshape</span>\n        x <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>bn3<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token comment\">#x = F.dropout(x, training=self.training)  # Apply dropout only during training</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fc2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> F<span class=\"token punctuation\">.</span>log_softmax<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\nnnModel <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>       <span class=\"token comment\"># Instantiate our model and move model to GPU if available</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model_parameters <span class=\"token operator\">=</span> <span class=\"token builtin\">filter</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> p<span class=\"token punctuation\">:</span> p<span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">,</span> nnModel<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nparams <span class=\"token operator\">=</span> <span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>np<span class=\"token punctuation\">.</span>prod<span class=\"token punctuation\">(</span>p<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> p <span class=\"token keyword\">in</span> model_parameters<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Model size: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>params<span class=\"token punctuation\">}</span></span><span class=\"token string\"> parameters'</span></span> <span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Model size: 213891 parameters</code></pre></div>\n<h2>Objective or Loss Function</h2>\n<p>In the model the final layer is a log-softmax function, in other words, we get a vector of per-utterance log probabilities or log-likelihoods. So the output of the model is basically\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{y} = \\log P(y | x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span></p>\n<p>If we combine this with the nll_loss, which stands for negative log-likelihood loss which basically does the following operation</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mo>−</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo stretchy=\"false\">[</mo><mi>y</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">l(\\hat{y},y) = -\\hat{y}[y]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">−</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">]</span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> is the true class label in <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><mi>C</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{0,1,...,C-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\">1</span></span></span></span></span>. If the model predicted the correct command with probability 1.0, then this loss would be 0.0. Otherwise it would be a positive number. By combining log-softmax and NLL loss, we get the Cross Entropy Loss.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Define objective function</span>\nlossFn <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>nll_loss  <span class=\"token comment\">#When we combine nll_loss and log_softmax we get a cross entropy loss</span></code></pre></div>\n<h2>Model Optimization or Training</h2>\n<p>The new parameters are the previous parameters with a step (proportional to learning rate <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>) in the direction (negative gradient of loss function w.r.t parameters) that reduces the loss function. In terms of math for a single labeled example <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> and model prediction <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span><br>\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>n</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><mi>μ</mi><msub><mi mathvariant=\"normal\">∇</mi><mi>W</mi></msub><mi>l</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">W_n = W_{n-1} - \\mu \\nabla_{W} l(\\hat{y},y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.891661em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">μ</span><span class=\"mord\"><span class=\"mord\">∇</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span></p>\n<p>For a batch of examples, we simply replace the loss above with some reduction such as the mean loss over the batch.</p>\n<p>We will also apply <strong>learning rate decay</strong> on the initial learning rate. The StepLR function will apply a decay of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> every step_size number of epochs to the exisiting learning rate in the optimizer.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">learning_rate <span class=\"token operator\">=</span> <span class=\"token number\">0.1</span>   <span class=\"token comment\"># Learning rate for optimizer like SGD usually in [0.001, 0.1]</span>\n<span class=\"token comment\"># Define optimization</span>\noptimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>nnModel<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span>learning_rate<span class=\"token punctuation\">,</span> momentum<span class=\"token operator\">=</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Apply decaying Learning Rate</span>\nscheduler <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>lr_scheduler<span class=\"token punctuation\">.</span>StepLR<span class=\"token punctuation\">(</span>optimizer<span class=\"token punctuation\">,</span> step_size<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> gamma<span class=\"token operator\">=</span><span class=\"token number\">0.25</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Mechanics of Training</h2>\n<p>Basically has the following steps which we repeat until all batches of training data are consumed</p>\n<ol>\n<li>Get a batch of inputs (X) and corresponding labels (y), move to device</li>\n<li>Initialize gradients</li>\n<li>Calculate loss function on current batch of inputs and labels</li>\n<li>Calculate gradients by calling backward() on the loss function output</li>\n<li>Update parameters by calling optimizer.step()</li>\n</ol>\n<p>At the start of end of each epoch (when one complete round of training data has been used), the learning rate is decayed.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">train_losses <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\nTrainLen <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>sc_data<span class=\"token punctuation\">.</span>train_loader<span class=\"token punctuation\">.</span>dataset<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">train</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> lossFn<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> train_loader<span class=\"token punctuation\">,</span> log_frequency<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  model<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">for</span> batch_idx<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>train_loader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># Move to device</span>\n    X_train<span class=\"token punctuation\">,</span> y_train <span class=\"token operator\">=</span> X_train<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Initialize gradients</span>\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Predict on current batch of data</span>\n    y_hat <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Calculate Average Loss</span>\n    loss <span class=\"token operator\">=</span> lossFn<span class=\"token punctuation\">(</span>y_hat<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Calculate Gradients</span>\n    loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Update model parameters using SGD</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> batch_idx <span class=\"token operator\">%</span> log_frequency <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Train  </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>batch_idx <span class=\"token operator\">*</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">/</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>TrainLen<span class=\"token punctuation\">}</span></span><span class=\"token string\"> Loss:</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\\n'</span></span><span class=\"token punctuation\">)</span>\n      train_losses<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n  scheduler<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># update learning rate for next call to train()</span></code></pre></div>\n<h2>Let’s Train</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">val_losses <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\nn_epochs <span class=\"token operator\">=</span> <span class=\"token number\">50</span>\nval_accuracy <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>n_epochs<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nlog_frequency<span class=\"token operator\">=</span><span class=\"token number\">100</span>\nval_accuracy<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>_ <span class=\"token operator\">=</span> evalModel<span class=\"token punctuation\">(</span>nnModel<span class=\"token punctuation\">,</span> lossFn<span class=\"token punctuation\">,</span> sc_data<span class=\"token punctuation\">.</span>val_loader<span class=\"token punctuation\">,</span> val_losses<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_epochs <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Epoch-</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>epoch<span class=\"token punctuation\">}</span></span><span class=\"token string\"> lr: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>optimizer<span class=\"token punctuation\">.</span>param_groups<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"lr\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n  train<span class=\"token punctuation\">(</span>nnModel<span class=\"token punctuation\">,</span> lossFn<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> sc_data<span class=\"token punctuation\">.</span>train_loader<span class=\"token punctuation\">,</span>log_frequency<span class=\"token punctuation\">)</span>\n  val_accuracy<span class=\"token punctuation\">[</span>epoch<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>_ <span class=\"token operator\">=</span> evalModel<span class=\"token punctuation\">(</span>nnModel<span class=\"token punctuation\">,</span> lossFn<span class=\"token punctuation\">,</span> sc_data<span class=\"token punctuation\">.</span>val_loader<span class=\"token punctuation\">,</span> val_losses<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Val set: Avg. loss: 0.0007, Accuracy: 94.9005126953125 %</code></pre></div>\n<h2>Performance Evaluation</h2>\n<p>The train dataset was used to train the model. The validation dataset is used to see if our model is overfitting or not over epochs. The test set is only gets used once at the end of model training and any architecture or hyperparameter optimization.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 508px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/14fa3709be47c4f18d2d6e8f90af1afb/2fd48/output_22_1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 79.05405405405406%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAk6AAAJOgHwZJJKAAACHklEQVR42p1STW/TQBA1cAF+G+K/IE6o4kNCiENbOCAOoMIFQVMK/UhSAlVFQQqVqkYCcgOpOLHz4cRxEuzN7qa297G7dtJE9JBmrOeZ2Td6np2xYVlNo932HhJCfzFGDymlpZnBaKnrDkoV0z6qVMyfnuflDQCG4wRbnGMuIwMf9UYNlmXBdV1HC9ZqvXXGFC2GQoj4PIjkIy2UMQghplEut+WVBxlKhVJUhJi1u4nSWL0451XdoW13N9IOZxJUJaOyNNaCjLFEsNPhq/N0OPGB0w6lXWi1yBpjYtzhqINzYEpQbnmwFpBEEJKfY9nTM6zX2arnaiKUOxNycwjj8DyIVR/jK7vtYN2sB3CpEzp+SziBg6bfhPI6nsBZZw2/EbOQYciHVcNtW0YU0kzhSxe9gIVqxKfTToFpL8SET6+s4vGWgd77t5sES4sISaAogTMh/j8bLUVRUZTOkLIg0+oHuLUAfu06ohs3Ed2+i2jpEaInTxHduYdo+TGi+w8Sv/IyOVO88ovLOHn2HOh02B8t2HSdXHjyF/FgiKMiw8E+x4dNjnevOYp7DN8+J/nXXYZPWa6x8Ybj1QrH3g7X/G4hhufxjhasmMcLfbu2Ty1rC76dRb+SR1DNgco4qG3rXMXE3gaRuW/uYGBlwSWIlYN7nJdjK/hB/4X+D5M5SpSLBvhBmv/QXtqVFFclLqf+IvyPkv8u8Tutb6naS/8A1rR5ScReFikAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/14fa3709be47c4f18d2d6e8f90af1afb/2fd48/output_22_1.png\"\n        srcset=\"/static/14fa3709be47c4f18d2d6e8f90af1afb/12f09/output_22_1.png 148w,\n/static/14fa3709be47c4f18d2d6e8f90af1afb/e4a3f/output_22_1.png 295w,\n/static/14fa3709be47c4f18d2d6e8f90af1afb/2fd48/output_22_1.png 508w\"\n        sizes=\"(max-width: 508px) 100vw, 508px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>As usual we will evaluate performance using accuracy rate, i.e. how many commands do we get correct on average. Another useful metric to look at is the confusion matrix, which tells us about how the model makes errors, when it makes errors.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">test_loss <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\naccuracy<span class=\"token punctuation\">,</span> cm <span class=\"token operator\">=</span> evalModel<span class=\"token punctuation\">(</span>nnModel<span class=\"token punctuation\">,</span> lossFn<span class=\"token punctuation\">,</span> sc_data<span class=\"token punctuation\">.</span>test_loader<span class=\"token punctuation\">,</span> test_loss<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Test set accuracy = </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>accuracy<span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Test set: Avg. loss: 0.0007, Accuracy: 94.42277526855469 %</code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/fdd383bd5ac698167850e7bd06abd7c3/f7616/output_26_1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 84.45945945945947%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAxNAAAMTQHSzq1OAAADT0lEQVR42l1UW0gUURg+m+ZtDR+kvFQaKIUhKVGWht00NVOKHnrQEqUe08QiAnsQtJdI8l7ggw/q2osY+SBEUZLobhFUREZG4m13Z93Z+467s7Nz+v+zZze3gY//zH/O/53vv8wQl0w1QUqJ4cvisaUVU5NgddQZBbHebHXUrxkt199/mG9GzM4t3F4zWW6YLLZ6IwBsnWnT3rS8ajxOIT6p4NaphPzGBAIvsej48Xv9mUn0UPdWgLokOWIlWWXw+BSG8F4YFptzmBGWtk/GFTRmEZUTCqLzqVMKULND8gGB4vUrChAEvf4gA67Bj2sF94HMjxdYHe4BjN91ZUgXd7QhJ6IQ0uljqrYUGZSqLimgSv6gCgQqBDLgGhSGfQE8v2lzDWJ8etO4LvXqk8wIIdSjB2+E87LdI6s2t5+i4q2ASoGAIZw6WngPoA8UMsJDbVMTaZUtuUgYg47VDaEfb8SDqEx0+ajFIUHNAoyE7zHL6xmlsLjrta7w7mgGKmMKjRaxFw9CkIzpwVlq98rUZPNS4KR8L2LxYlxv2kMKawZmJ47UP8glFyqqUngN+/lhTEUNp+n2BanRht1XIilHE4YU3hz7pLvWM51BWtvuJaJjQxB7WVo+ReZNgOBQenaPn6XvhJHx8vEJpyyIDkb48NXXido7nf9quG7e7INRARXRCiXeDBGaZPPIrFFRTeEpP5/9qesYm8kkcFlsSKG1N9wUPiJRdfPAHpIKdol6tqUM88sIJz/+mWjp6P6nEFMONwUVchUR4B42ChWaRC+qDLjBZ7KGCGe/b+hGpt7ujRCubJj7OKEPLH4JDPhl4DsC0mc+GH4Fht9PQw/7Uj4vCeNd3UM5ZFT3Ih4dZqt9cFsH/x+R7QMNUCmqG3mpp82PdMOtA3rya902OTXzbj+5WH1Ji4RLy2tt8IdZhEGdg7roYRwW4I9iwDX4DTCvBmjAAq4F0aV3e6X5sw2Pv5Gk6vuEFBH4U3VOz7zZQzQaDeHPDkA8IAYQC0gAJPJ1HLcx/AxiZ+HBtOT8nLTkrPQUjNdwEKLVaklFZRUpLikhNbWXSenpM8yfnX2A8GCMiM/LO0zOnS8jRSdOkvLyMlQQn5+tJakpiaS9uYnsS99N/gJ9hgMJk3FanQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/fdd383bd5ac698167850e7bd06abd7c3/fcda8/output_26_1.png\"\n        srcset=\"/static/fdd383bd5ac698167850e7bd06abd7c3/12f09/output_26_1.png 148w,\n/static/fdd383bd5ac698167850e7bd06abd7c3/e4a3f/output_26_1.png 295w,\n/static/fdd383bd5ac698167850e7bd06abd7c3/fcda8/output_26_1.png 590w,\n/static/fdd383bd5ac698167850e7bd06abd7c3/f7616/output_26_1.png 766w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Final thoughts</h2>\n<p>We ended up with a final test accuracy of around 94.4% with our 200k parameter model. Seems not too shabby, considering that the baseline method in the <a href=\"https://arxiv.org/pdf/1804.03209.pdf\">SpeechCommands paper</a> results in an accuracy of 88.2% with the best model, which I think is referred to as <em>cnn-trad-fpool3</em> with 250k parameters. I’m not exactly sure since they don’t explicitly mention those details in the paper and refer to another paper from 2015, which itself has more than a few models.</p>\n<h4>Next steps to improve performance</h4>\n<ol>\n<li>The idea of a background class seems a bit strange. It definitely makes the data balance off and model could cheat by just saying something is background</li>\n<li>In background, we probably can differentiate between silence, noise (type of noise), speech, etc. This may result in better representations</li>\n<li>Better model architecture: like ResNets, lower parameter count / complexity.</li>\n<li>Better training: multi-task learning, self-supervised learning</li>\n<li>Built in Denoising, Dereverb, Speaker Normalization</li>\n</ol>\n<h4>Wishlist</h4>\n<ol>\n<li>Use this model on my voice on real-time audio stream in a browser and study generalization on different microphones</li>\n<li>Capability to train (finetune) the model in an online (supervised) fashion</li>\n<li>Understanding model predictions and debug methods for misclassified examples</li>\n</ol>","excerpt":"This is a tutorial post on speech commands recognition using the Speech Commands dataset. The goals for this post Work with audio data using torchaudio: look at…","frontmatter":{"title":"Speech Commands recognition using ConvNets in PyTorch (Tutorial)","date":"2020-08-10T00:00:00.000Z","subject":["ml","tutorial","pytorch","audio","speech","kws","voice","cnn"],"author":"RSP","featimg":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAxNAAAMTQHSzq1OAAAFWUlEQVR42i1UeUwUdxT+ZgaWw0UOa2ChViOkwVoFikZRW4zBWmtRIW2JWPEACVoFFawsLMslcshRPBCItRWkUI+gAiIeoSkWVjyQW5CKyGHwQmUR2J2d1x+0f7zM/N7vfd+843sDcC3h4B9mA81pcHt6BF88zYNX33H5zr503rUvHahPh8kdB2CNBRB9GFBlAJGH4X3lqLFdfJ5fy8GfnVvb0qC4kcVhnxqQdfdAeEQQOolfOUwWSa9IHvqS7KIek2z1SwK6CFNaFzBCBRBL/5mKTHf8TcYzj1NJoxep3iSTuVcl8ytHAZP2+xAa2aF9zDz+rc599Jbepa5W90F6z7hpwHM90KbjrOpcgfW2jGyMmcgIx00XFupgeUSvatil+67+3DhkFcQjogdwH2jEjEHCnFdi2JNUKZHCJauLA5JR1FuDUdAQ+1CzBKsmN5YhI1QzMjXzxRiATAl8iuR7OUfa8vJXA+9SxfxR/eC/GW2G0zg57qinwRpr6imyI59bv9P88tukyHnCgtpIMKpxAx9gBySycwKzeILZRTKfmkJNznLSFXHkXZTH/AXPIHw7chsrSVwdUj5MadBSNbT6OzKtn7ZAa1M2MIKVb7TC3Lsu8GywhX3FOyD7PbgSLeRXtCbGJ7X3PGdoaTOGi6o3jGN+cxc4z3NBmFWutvc4Erl904pYn9A1h3y3rUswXVGrhGtvFL+6TYnd/azczVNgnBYJHIhipSk5IT5B/nXmobW/ZcUHZv4Y6blZreIsM3YD+KUJXBlL9yLryx/sWcjsLHHyJ5LgoGXvLwguIuvhYlZyjAgujvmiJUy/QH5laeTTyzD7+g0wPc/8cf2AcUUTTOsI3J8GHsXEG1cRz5ezS60EBZFs7xBZlb1hhMvtwMUaJgg5KCVMK6X9JSGUT1vI5uhjA+bUMsyZAZZhRSNwmWB534DpjRJQM2nLgs9LqksJtGH0JNlQHyP0tgMfNzllAVEs5pSUfepLqYucJB8qNDhcf0oW/t0sQ/OWBlh0ED4a1EHxjJXUJgpzO0TNjQV62gFDaYOXuIZKmQ5DbDk+ScemLAlI0ENWLMaGB4hSPMSH92fqKmg5ZVJwL/DD6D9ciIHgrSNuIevZZ2M0dcNrurd+Fg06cjQcaEyUB3egWMFPlCswybAsrfdepfSze6g92I5uVH5Oh/pDKag+lakgrHs3FziYioAXB7GkLQNujZlWbjeyzoe5JV+NW5RUGr0k6Z6/PVu7ODnHhR8EfkoCdh3irHIy7VKvZc4uvpkl39RwEAtvJsOxMhImn5RcYEEdQDjrZUQbsK8d2MsstxlIYaZuAvY7m+H5NLi8b+S/H23h9w83W+Q8bofvcDscLrDYUIZVtrPVuwn7rbWdDEzgWSlc4uQWyKzzyDG4kqx9bpJgfIzMhAB3WJECG/XEJ+qJU4k0r+UuLXlURxbeEzJTEowm8OohLEj66wFkVUwKyXqBZ7JAsiE4JdxAr83EY31hEma3Mt9OV/SS7YedHfrFw9VkP9wlFo75GMppleHToAmMWuSEyR3vw7Lc6w8gXGNSiNbxUImQ5YslQR7iYL6l+DrbwhB9IkJ023Pa1Ymu2y4dq9Ipx2KkY+3bxe679mLaSIRo5F/CpHRAD2FC8DG9WJd7phMe3eyfVzy5JX7+ifTuK5AuBhPTpZrWRTT7fb27ouiaYjFV05n6tURL2Z0HqPCOH8kDCxgukv4nHEJ29ZairZfyGjam5tatzijUNJ5w1pASmlen5ZqRo9zte0VOmtQrAR9jY4cNn/ms1jcjv74ue57mQcxszYmqbRqTVQUaNtQ68DF32Y6X/wsE3shC83m7ugAAAABJRU5ErkJggg==","aspectRatio":1.0638297872340425,"src":"/static/842d14906ffb1a2c699e465c17560cca/497c6/output_5_0.png","srcSet":"/static/842d14906ffb1a2c699e465c17560cca/65e33/output_5_0.png 100w,\n/static/842d14906ffb1a2c699e465c17560cca/69585/output_5_0.png 200w,\n/static/842d14906ffb1a2c699e465c17560cca/497c6/output_5_0.png 400w,\n/static/842d14906ffb1a2c699e465c17560cca/2a4de/output_5_0.png 600w,\n/static/842d14906ffb1a2c699e465c17560cca/ee604/output_5_0.png 800w,\n/static/842d14906ffb1a2c699e465c17560cca/b542f/output_5_0.png 1164w","sizes":"(max-width: 400px) 100vw, 400px"}}}},"fields":{"slug":"/tutorial-pytorch-speechcommands/output/","readingTime":{"text":"8 min read"}}},"site":{"siteMetadata":{"title":"JumpML"}}},"pageContext":{"slug":"/tutorial-pytorch-speechcommands/output/"}},"staticQueryHashes":["2046284594","429448491"]}