{"componentChunkName":"component---src-templates-blog-post-js","path":"/design-ml-library/","result":{"data":{"markdownRemark":{"html":"<p>This article is an attempt to think about all the various activities in\na deep-learning based ML model development process and to distill those\ninto a high-level functional and object-oriented design library based on\nthe PyTorch framework.</p>\n<p>At a high level, the process of ML model development for a single task\ncan be subdivided into the following stages</p>\n<ol>\n<li>Data Setup and Feature Preprocessing: perhaps multiple datasets,\naugmentations,</li>\n<li>Model and Objective/Loss Specification: specified in code in PyTorch</li>\n<li>Model Training: optimization</li>\n<li>Performance Evaluation: performance metrics on validation set, test\nset</li>\n</ol>\n<p>The goals of the new high-level library would be to support and simplify</p>\n<ol>\n<li>Model specification: modular ability to combine multiple models like\nlegos</li>\n<li>Loss-function specification and tuning: each layer can have\ndifferent and multiple objectives, can arrive from models themselves</li>\n<li>Model deployment tools: quantization, factorization, ONNX etc.</li>\n<li>Self-supervised learning and coupled with data augmentation: support\nfor online self-supervised learning, test-time self-supervised\nlearning</li>\n<li>Multi-task learning: train on multiple datasets and tasks. Some\ntasks can share common representations and have task specific\nbranches subsequent to that</li>\n<li>Curriculum learning: a teacher (RL agent, perhaps) figures out\nsequence of tasks and examples within those tasks to learn better\nrepresentations and/or perform better at tasks</li>\n<li>Multi-mode learning: when inputs are from two or more different\ndomains (like audio, sensors, video, images, etc.) and ability to\nshare/relate representations of same object</li>\n<li>ML model debugging: feature attribution, hard examples,\nout-of-domain indicators</li>\n<li>Model report card</li>\n</ol>\n<p>Let’s take all of these goals and then try to create an example that\nallow easy use and expression of high-level ideas quickly.</p>\n<h2>High-level example using JumpML library</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> jumpml\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n\n<span class=\"token comment\"># datasetFile defines</span>\n<span class=\"token comment\">#         multiple datasets, train, test, validation splits</span>\n<span class=\"token comment\">#         feature processing pipelines (could include pretrained models themselves)</span>\n<span class=\"token comment\">#         transformations and augmentations</span>\n<span class=\"token comment\">#         curriculum learning options</span>\ndataloader <span class=\"token operator\">=</span> jumpml<span class=\"token punctuation\">.</span>createDataloader<span class=\"token punctuation\">(</span>datasetFile<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># modelFile defines</span>\n<span class=\"token comment\">#          pytorch modules and connections</span>\n<span class=\"token comment\">#          objective functions at/between layers/nodes</span>\nmodel <span class=\"token operator\">=</span> jumpml<span class=\"token punctuation\">.</span>loadModel<span class=\"token punctuation\">(</span>modelFile<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># define optimizer as usual.</span>\noptimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span>learning_rate<span class=\"token punctuation\">,</span> momentum<span class=\"token operator\">=</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Create Learner based on learnerOptions</span>\n<span class=\"token comment\">#          initialization, EMA parameters, EWC, parameter saving</span>\n<span class=\"token comment\">#          self-supervised learning phases</span>\n<span class=\"token comment\">#          curriculum learning based on learning</span>\n<span class=\"token comment\">#          dynamic model augmentation: adding more capacity or branches dynamically</span>\n<span class=\"token comment\">#         logging learning progress</span>\nlearner <span class=\"token operator\">=</span> jumpml<span class=\"token punctuation\">.</span>createLearner<span class=\"token punctuation\">(</span>dataloader<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> learnerOptions<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Self-supervised phase</span>\nlearner<span class=\"token punctuation\">.</span>ssLearn<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Supervised Learning Phase: perhaps trains only the task-specific layers... may be does some multi-task learning on some other layers.</span>\n<span class=\"token keyword\">for</span> epochs <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>numEpochs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    learner<span class=\"token punctuation\">.</span>trainEpoch<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># this could only impact the classifier branches</span>\n\n\n<span class=\"token comment\"># Evaluation</span>\n<span class=\"token comment\"># evalMetrics are a list of functions.</span>\n<span class=\"token comment\"># each function takes model predictions, ground truth and calculates metrics</span>\nevalResults <span class=\"token operator\">=</span> jumpml<span class=\"token punctuation\">.</span>evalModel<span class=\"token punctuation\">(</span>dataloader<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> evalMetrics<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Reports, visualization, deployment</span>\n<span class=\"token comment\"># may run a separate distillation loop (quantization, factorization, onnx exporting)</span>\njumpml<span class=\"token punctuation\">.</span>deployModel<span class=\"token punctuation\">(</span>evalResults<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> deploymentConfig<span class=\"token punctuation\">)</span></code></pre></div>","excerpt":"This article is an attempt to think about all the various activities in\na deep-learning based ML model development process and to distill those\ninto a high…","frontmatter":{"title":"Design of a high-level PyTorch ML library","date":"2020-07-30T00:00:00.000Z","subject":["pytorch","ml","sw","design","opensource"],"author":"RSP","featimg":null},"fields":{"slug":"/design-ml-library/","readingTime":{"text":"3 min read"}}},"site":{"siteMetadata":{"title":"JumpML"}}},"pageContext":{"slug":"/design-ml-library/"}},"staticQueryHashes":["2046284594","429448491"]}