{"componentChunkName":"component---src-templates-blog-post-js","path":"/perspectives-intelligence/","result":{"data":{"markdownRemark":{"html":"<p>These are just some of my thoughts on machined-learned intelligence,\nformed by reading a bunch of papers and watching a few talks here and\nthere.</p>\n<h3>Intelligence, tasks, representations and learning</h3>\n<p>Taking inspiration from Wikipedia, intelligence or intelligent behavior\nmay be decomposed into these capabilities</p>\n<ol>\n<li>Perceive or infer information</li>\n<li>Retain that information into knowledge for easy reuse (store and\nfactorize experience)</li>\n<li>Apply this knowledge towards adaptive and preferred behaviors within\nan environment or context</li>\n<li>Ability to measure success and learn from decisions by exploring\nopportunities to improve the steps above</li>\n</ol>\n<p>In <a href=\"https://arxiv.org/pdf/0712.3329.pdf\">Legg2007</a>, a quantitative definition of intelligence is proposed,\nwhere the objective (referred to as Universal Intelligence) is\nessentially maximized when for each task, the agent tries to minimize\nthe computational complexity of learning and inference of agent policies\nfor each task. In essence, to solve each task efficiently.</p>\n<p>A <strong>task</strong> is making a decision (or sequence of decisions) based on the\ndata. Decisions could be classifications, predictions and actions. A\n<strong>representation</strong> is a function of the data which is useful for a task.\nUsually the input data dimension is very large (pixel space, sampled\nwaveforms, etc.) and the target space (classifications) are in a much\nlower dimensional space. <strong>Learning</strong> is the process of using data (or\nexperience) to figure out good representations to solve the task.</p>\n<h3>What are good representations?</h3>\n<p>Let <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span> denote the input data, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> be a function that maps the input\nto a representation <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">z = f(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> which is designed so that the task,\ndenoted by decision (random variable) <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> can be solved effectively. The\nmapping <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> is learned via data and tasks. We would like that the\nfeatures or representations learned help efficient future task learning\nand ideally have the following attributes</p>\n<ol>\n<li>Invariant to nuisances</li>\n<li>Disentangled, independent, composable, reusable, interpretable</li>\n<li>Compressed, minimal, sparse</li>\n<li>Hierarchical, multilevel</li>\n<li>Task agnostic</li>\n<li>Dynamically added via life-long learning</li>\n</ol>\n<p>We would like the representation to be invariant to nuisances, such as\ntranslation, rotation, change of scale or lighting, noise, that are not\nrelevant to predicting <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span></p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>g</mi><mo>∘</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x) = f(g \\circ x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∘</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>Disentangled or independent representations allow composition of factors\nof variation to create new unseen concepts (imagination) and allow\nabstract reasoning. These factors correspond to finding invariants and\nindependently transformable aspects of the world. Tasks are defined in\nterms of these invariant entities. New concepts can be formed from\nlogical combination of old concepts. For example, a blue orange.\nDisentangled factors of variation can be used for one-shot or zero-shot\nlearning. New factors can be discovered and added in a life-long\nlearning setting.</p>\n<p>In order to reason about concepts or imagine scenarios, it is essential\nto have a representation which is also sparse. We don’t want to reason\nabout things that are irrelevant to the task at hand. We need to be\nreally good at discerning irrelevant information and throwing it away.\nSometimes the previously trained representations may need to be adapted\nto provide this property.</p>\n<p>As we get more abstract and form higher-level concepts, it is necessary\nto throw away information and only model things about the objects in the\ninput that are crucial for the task. This not only makes it more robust,\nbut also allows reasoning at this higher level of abstraction. It is\nlikely easier to model the world (physics) at this level as well. We\nwould like the representation <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> to have enough information to solve\nthe task and simultaneously minimize information about the data that is\nnot relevant for the task <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>. Stated mathematically using the concept\nof mutual information and referred to as the <em>Information Bottleneck</em></p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><munder><mo><mi>min</mi><mo>⁡</mo></mo><mi>f</mi></munder><mi>I</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">;</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mi>λ</mi><mi>I</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">;</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\min_{f} I(f(x);x) - \\lambda I(f(x); y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.638216em;vertical-align:-0.8882159999999999em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.66786em;\"><span style=\"top:-2.3478920000000003em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10764em;\">f</span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\">min</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8882159999999999em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">λ</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>We should note that one task’s nuisance is probably another task’s\ndiscriminative data. I’m thinking of a speech recognizer and an\nenvironment (noise) sound classifier, for example. There is some\ninteresting work <a href=\"https://arxiv.org/pdf/1802.07088.pdf\">Jacobsen2018</a>\nabout invertible neural networks where\nthey show that compression (throwing away information, except at the\nlast classifier layer) is not required for achieving high classification\nperformance. Based on this we are speculating representations to have\nsome sort of hierarchy:</p>\n<ol>\n<li>Invertible task-agnostic features: no information loss</li>\n<li>Intermediate task group features: information loss via gating,\nmasking or attention mechanisms</li>\n<li>Task-level features: high-level disentangled abstract latent space</li>\n</ol>\n<p>As an example of hierarchical representations, we can have at highest\nlevel (deepest layer) the concept of living organism, then at the next\nlower level, the concept of bird and then the level below that the\nconcept of crow and bluebird and then beaks, wings, legs and so forth.\nWe could ask, what are similarities between different examples of a\nclass and these could be color, size, shape differences. As we go deeper\ninto the hierarchy we lose details about the input that aren’t relevant.\nSo if we are interested in cat detection, all information about the\nbackground could be thrown away by the time we reach the cat concept.</p>\n<h3>Learning and prediction</h3>\n<p>Deep learning has been successful in various tasks. The model parameters\nthat are the weights of the layers of the neural network are learned via\nbackpropagation using stochastic gradient descent (SGD). If we have good\nhyperparameters (initial weights and learning rates) and sufficient\ntraining data, SGD can often provide good representations that\ngeneralize. Transfer learning via fine tuning the final layer and/or\nrepresentations is a practical method of taking a network trained on one\ntask (for its representations) and then applying it on another where the\ndata is not as plentiful. Meta learning addresses the problem of how to\nefficiently learn a new task, given experience learning various tasks.\nIn life-long learning we need to detect shifts in data distribution. We\nneed to prevent catastrophic forgetting. We need to allocate spare\nrepresentation capacity to learn new concepts and share or consolidate\nlatents where appropriate.</p>\n<p>The concept of compute adaptive prediction, in order to effectively\nfocus on the right information for the task, there may be parts in the\nprediction or representation that are not fixed steps and may be\niterative for complicated inputs. This could be a sequential process\ndriven by RL-based policy. Humans take more time when trying to find an\nobject that is camouflaged or speech in low SNR, for example. In an\nenvironment with a time aspect, we want to predict multiple outputs for\na single input and learn when the actual observation is divergent from\nour predicted future. Having a concept of prediction with constraints\nallows to produce outputs that satisfies certain task-dependent\nconstraints, such as a linguistically correct sentence. We can use\nenergy/barrier functions for prediction with constraints</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo>=</mo><munder><mo><mi mathvariant=\"normal\">arg min</mi><mo>⁡</mo></mo><mi>y</mi></munder><mi>F</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{y} = \\operatorname*{arg\\,min}_y F(x,y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.780548em;vertical-align:-1.030548em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.66786em;\"><span style=\"top:-2.20556em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\"><span class=\"mord mathrm\">a</span><span class=\"mord mathrm\">r</span><span class=\"mord mathrm\" style=\"margin-right:0.01389em;\">g</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathrm\">m</span><span class=\"mord mathrm\">i</span><span class=\"mord mathrm\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.030548em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>As an example of learning disentangled factors from data, we have\nindependent generative factors like position, size, shape, rotation and\ncolor. Then the generated image is an example of entangled data. In\n<a href=\"https://openreview.net/pdf?id=Sy2fzU9gl\">Higgins2017</a> a model\ncalled BetaVAE was proposed to learn the\ndisentangled representations. The BetaVAE model is able to learn\ndisentangled representations by modifying the loss function with\nadditional terms that encourage independence of the latent\nrepresentations. A key concept in learning good representations is\nconstructing objective and loss functions. These representations allow\nto traverse across the latent dimensions.</p>\n<p>Curriculum learning is a well-designed sequence of tasks that enables\nreuse and good representation learning. Open question is how to design\nthese automatically.</p>\n<h3>Final thoughts</h3>\n<p>For now, we will superficially list a bunch of concepts or buzzwords\nthat seem to be relevant to the design of intelligent systems, this is\nthe TL;DR version if at all</p>\n<ul>\n<li><strong>Gradient-based learning</strong> are learning algorithms that use the\ngradient of the objective function or loss function with respect to\nthe differentiable model parameters to nudge the model towards a\nbetter performance on the task</li>\n<li><strong>Continual or life-long learning</strong>: is the idea of the model being\ncapable of learning to do new tasks and avoid the problem of\ncatastrophic forgetting, where the model forgets about tasks it\nlearnt in the past. Controlled forgetting is when the model\npurposefully forgets what has been learned in order to learn\nsomething new in a new situation faster, perhaps a more compact\nfactorization of knowledge.</li>\n<li><strong>Self-supervised learning</strong>: learning representations without the\nlabels is termed <em>unsupervised learning</em>. Recent work in NLP and\nVision have shown that good low level features can be learned from\neven a single representative example using augmentation and\nappropriate self-supervised tasks</li>\n<li><strong>Specialized modeling</strong>: For particular instances in a set of\ntasks, it may be beneficial to route it to a more specialized model\nto provide higher capacity. It helps to understand the environment\nin which we are in to figure out best modeling options for the\ndownstream task. Alternately, this could be a special preprocessing\nstep on the instance to adapt its domain</li>\n<li><strong>Memorization</strong>: special case handing for useful concepts and\nexceptions (OOD) to enable faster adaptation when domains switch.\nClustering and exemplars.</li>\n<li><strong>Multilevel features</strong>: global vs. local perspective, using\nattention to dynamically focus on different aspects sequentially</li>\n<li><strong>Metalearning</strong>: figuring out whether transfer learning (final\nstage linear classifier), rapid learning, good initializations,\nadding spare capacity (new representations), decide what\nrepresentations to share, those to modify in a proper way are needed\nto learn new tasks efficiently</li>\n<li><strong>Disentangled representations</strong>: to enable wider generalizations\nbeyond interpolation to nearest example, there needs to be high\nlevel semantic features that are sparse, independent, disentangled,\nwhere notions of similarities, new combinations are easily performed</li>\n<li><strong>Generative modeling</strong>: models learned in the abstract disentangled\nlatent space will allow simulation of futures and allow high-level\naction planning. These could be trained via self-supervised\npredictive coding.</li>\n</ul>","excerpt":"These are just some of my thoughts on machined-learned intelligence,\nformed by reading a bunch of papers and watching a few talks here and\nthere. Intelligence…","frontmatter":{"title":"Perspectives on machine-learned intelligence","date":"2020-07-16T00:00:00.000Z","subject":["ml","perspectives"],"author":"RSP","featimg":null},"fields":{"slug":"/perspectives-intelligence/","readingTime":{"text":"8 min read"}}},"site":{"siteMetadata":{"title":"JumpML"}}},"pageContext":{"slug":"/perspectives-intelligence/"}}}