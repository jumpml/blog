{"componentChunkName":"component---src-templates-blog-post-js","path":"/tutorial-pytorch-mnist/Pytorch_MNIST/","result":{"data":{"markdownRemark":{"html":"<p>In this post we will get acquainted with the basics of training a machine learning model using PyTorch. ML model development can be subdivided into the following stages </p>\n<ol>\n<li>Data Setup        </li>\n<li>Model and Objective/Loss Specification     </li>\n<li>Model Training</li>\n<li>Performance Evaluation</li>\n</ol>\n<p>In this tutorial, our goal is to quickly train a CNN (convolutional neural network) model going through all the four stages. The idea here isn’t necessarily to introduce CNN or ML, but it is to get used to doing things in PyTorch. We will use the <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST dataset</a>, which is considered to be an easy problem. </p>\n<p>This notebook is available on github at this <a href=\"https://github.com/jumpml/pytorch-tutorials/blob/master/Pytorch_MNIST.ipynb\">link</a></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># CUSTOMARY IMPORTS</span>\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torchvision\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>functional <span class=\"token keyword\">as</span> F\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n\n<span class=\"token operator\">%</span>matplotlib inline</code></pre></div>\n<h2>Data  Setup</h2>\n<p>In PyTorch we have a few concepts that can help ease the data setup process  </p>\n<ol>\n<li><strong>Dataset</strong>: each dataset (like MNIST) will have a class which implements __getitem__() which returns an example (tuple)</li>\n<li><strong>DataLoader</strong>: takes a Dataset as input and outputs a generator or iterable object. One can use next() on a DataLoader object to get the next example from the dataset. The DataLoader can be setup to return a batch_size number of examples.</li>\n<li><strong>Transforms.Compose</strong>: applies a list of transforms from left to right on each example(s) that is returned </li>\n</ol>\n<p>Transforms can be used to augment the dataset by applying transformations such as scaling, rotations, masking, etc. For MNIST, the torchvision package has already implemented a Dataset called MNIST, where it will download the data for us automatically, if not already downloaded. Since the MNIST data is split up into different files, we also need to specify whether we wish to setup a train set DataLoader or a test set DataLoader. </p>\n<p>Prior to data setup, let us also define some training hyperparameters. These are not parameters in the classical sense, but they do impact the solution that we end up at. Please read the comments for more details on what they do.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># TRAINING HYPERPARAMETERS: </span>\nn_epochs <span class=\"token operator\">=</span> <span class=\"token number\">3</span>           <span class=\"token comment\"># How many passes through the training data  </span>\nbatch_size_train <span class=\"token operator\">=</span> <span class=\"token number\">64</span>  <span class=\"token comment\"># Training batch size usually in [1,256]</span>\nbatch_size_test <span class=\"token operator\">=</span> <span class=\"token number\">1000</span> <span class=\"token comment\"># Test batch size (choose anything)</span>\nlearning_rate <span class=\"token operator\">=</span> <span class=\"token number\">0.01</span>   <span class=\"token comment\"># Learning rate for optimizer like SGD usually in [0.001, 0.1]</span>\nlog_frequency <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\nrandom_seed <span class=\"token operator\">=</span> <span class=\"token number\">1</span>        \ntorch<span class=\"token punctuation\">.</span>manual_seed<span class=\"token punctuation\">(</span>random_seed<span class=\"token punctuation\">)</span>\ndevice <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token string\">\"cuda:0\"</span> <span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>is_available<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">else</span> <span class=\"token string\">\"cpu\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">train_loader <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>DataLoader<span class=\"token punctuation\">(</span>\n  torchvision<span class=\"token punctuation\">.</span>datasets<span class=\"token punctuation\">.</span>MNIST<span class=\"token punctuation\">(</span><span class=\"token string\">'./files/'</span><span class=\"token punctuation\">,</span> train<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> download<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n                             transform<span class=\"token operator\">=</span>torchvision<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>Compose<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n                               torchvision<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>ToTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                               torchvision<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>Normalize<span class=\"token punctuation\">(</span>\n                                 <span class=\"token punctuation\">(</span><span class=\"token number\">0.1307</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">0.3081</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n                             <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  batch_size<span class=\"token operator\">=</span>batch_size_train<span class=\"token punctuation\">,</span> shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\ntest_loader <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>DataLoader<span class=\"token punctuation\">(</span>\n  torchvision<span class=\"token punctuation\">.</span>datasets<span class=\"token punctuation\">.</span>MNIST<span class=\"token punctuation\">(</span><span class=\"token string\">'./files/'</span><span class=\"token punctuation\">,</span> train<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> download<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n                             transform<span class=\"token operator\">=</span>torchvision<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>Compose<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n                               torchvision<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>ToTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                               torchvision<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>Normalize<span class=\"token punctuation\">(</span>\n                                 <span class=\"token punctuation\">(</span><span class=\"token number\">0.1307</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">0.3081</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n                             <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  batch_size<span class=\"token operator\">=</span>batch_size_test<span class=\"token punctuation\">,</span> shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># The enumerate() method adds a counter to an iterable and returns an enumerate object</span>\nexamples <span class=\"token operator\">=</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>test_loader<span class=\"token punctuation\">)</span>\nbatch_idx<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>example_X<span class=\"token punctuation\">,</span> example_y<span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token builtin\">next</span><span class=\"token punctuation\">(</span>examples<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># VISUALIZE SOME EXAMPLES</span>\nfig<span class=\"token operator\">=</span>plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dpi<span class=\"token operator\">=</span><span class=\"token number\">60</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">9</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  plt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>tight_layout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>imshow<span class=\"token punctuation\">(</span>example_X<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> cmap<span class=\"token operator\">=</span><span class=\"token string\">'gray'</span><span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">\"Ground Truth: {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>example_y<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>xticks<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>yticks<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 532px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/30f9b6ba66e77895bd0b7d2efd96bd2e/89a37/Pytorch_MNIST_5_0.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 87.16216216216216%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAk6AAAJOgHwZJJKAAADZ0lEQVQ4y01U7UuTURR/fJlT5/uW4tyLOnzZ3FSU4XC5WbpypDhkflCcG4Q58YNIWLlVa4i9MBT8NkzyQ0SBQkFB+2JJEvTH9CEz2+amt995vM9ocLjnPr+dc3/n/s65gkKh2BcEgUlWUFBAq0O4/N0uKipi4+PjrLS0VMLDHLPAzxiNRoYcUvxnQafTvVlaWmJerzcdi8UuLBZLFoCdB/knJiaY0+nMlJeXn/X29lLQPY51FBcX/w4EAmxkZCRdVlZG2AcCXldXV7OFhYXk2tpaBvu0TCaTEs66XC7m8XhSkUgktbW1xYCtEIC1A8uvwsLC8+Xl5b+Dg4OU8D0Bn4ju9PQ0M5lMInW5XH6NJwzS3mq1srGxMdbe3k54lIC8vLwu6ZpQHdPr9eQfEnALTqSzs3MVZYXg34WpeUIT8IdY6fsqjHwrx5T5+flL9E2tVq+CxCP4HgLcsBjsKTcK0vIgMxLSt2cNDQ3rCCLfRgDEUmG5D3sBW4c9xwFewt4RbbfbzXAXbHR0lKgPEQAV5wmrr69n4XBYLA8HPOGHdZWUlLDu7m5ms9mkLvhCwHZLSwvb3NxMVVRUZHHJGQQNEAAVZykJDjpLJBIkFgWtSCrX1NQcU9vMzc2l2traLkXB76VKpaKgFFqAWiaLchw8oZ8SItkZYyyt1WpzCVE+qXyMw1k0Gk319PTkEr4lyqFQSFSTEoCJVPId2vf19bH+/n4Rwz1FOMNO2lOpw8PDUmMfEGCA3ZAMJRO7ch50BfshjrlwwPX/OqCEBgAMb6I7XPCH4RsFLv2PysrKA0zNIfwEAjt4ECU7wv4rBKALP4J5Oabjk/EN2AHu/zv8x2LJzc3NbHd3ly0uLkrUh/g0iCXb7Xbm9/tFDIxzJfO5ZzMzMzSeuZK3wZLt7Owkp6amSJQMhn2AX7yoMh6HtM/nS5EPwURRECOOHoS7iMfjyaqqqpwor+rq6tje3l52fn5eYujkogRoj5G7CAaD51yUB4QhgRnL6eTkJNvY2Mjyl+gjnRRHP9EsnzgcjiQYJMHsKmfoo+cLL8kpyv6DPc05jaYAIWgsf7a2tp4ZDIYTKh9s9wU0Zo1SqdTjPxoaOYDapqYmOQU1NjYqEKDTaDQ0iloMgB6Y2AG1tbUyPHUaPqYas9msx/9V/wB+Cifw7NOR7wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/30f9b6ba66e77895bd0b7d2efd96bd2e/89a37/Pytorch_MNIST_5_0.png\"\n        srcset=\"/static/30f9b6ba66e77895bd0b7d2efd96bd2e/12f09/Pytorch_MNIST_5_0.png 148w,\n/static/30f9b6ba66e77895bd0b7d2efd96bd2e/e4a3f/Pytorch_MNIST_5_0.png 295w,\n/static/30f9b6ba66e77895bd0b7d2efd96bd2e/89a37/Pytorch_MNIST_5_0.png 532w\"\n        sizes=\"(max-width: 532px) 100vw, 532px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Model Specification</h2>\n<p>All PyTorch models should inherit from the nn.Module (for automatic gradients and such). Building models in PyTorch is like Lego, we take some layers or operations like Conv2d, Linear, and then compose them in a function called forward (which is required to be defined). The forward function (also called prediction) takes the input and produces a output after processing through the model. The torch.nn.functional allows applying activation functions and dropout conveniently. We use the nn.module for layers that hold parameters and use the functional API for other operations like activations, softmax, etc. </p>\n<p>One of the challenges in model specification with CNNs and Linear or Fully-Connected layers is keeping track of the image sizes as it goes through the operations. It is important to understand that the tensor passed to the model (during training or prediction) is of dimension (BatchSize, NumChannels, H, W) and when we refer to image size we are talking about (H,W). It is common to refer to channels as filters or also kernels. </p>\n<p>The input or image sizes are non-trivial functions of parameters like kernel size, stride, dilation, padding change the size of the image. The easiest approach is to use the formula provided in the <a href=\"https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html\">Conv2d Documentation</a>. We will write a simple function to compute this, so that we don’t need to spend time thinking about this. This formula also applies to the max_pool2d layers. </p>\n<p>Our model is going to be simple:<br>\nX (B,1,28,28)<br>\n—> Conv2D(1,32,5) —> MaxPool2D(2) —> ReLU<br>\n—> Conv2D(32,16,3)—> MaxPool2D(2) —> ReLU<br>\n—> Linear(100)    —> Dropout(0.2) —> ReLU<br>\n—> Linear(10)     —> LogSoftmax  </p>\n<p>where B is Batch Size, the arguments to Conv2D are input numChannels, output numChannels and kernel size.  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">get_conv_size</span><span class=\"token punctuation\">(</span>in_dim<span class=\"token punctuation\">,</span> kernel_size<span class=\"token punctuation\">,</span> stride<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> dilation<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">(</span> <span class=\"token punctuation\">(</span>in_dim <span class=\"token operator\">+</span> <span class=\"token number\">2</span><span class=\"token operator\">*</span>padding <span class=\"token operator\">-</span> dilation <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span>kernel_size <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> stride  <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">input_size <span class=\"token operator\">=</span> <span class=\"token number\">28</span>    <span class=\"token comment\"># MNIST has 28 x 28 input images</span>\nC1_kernel_size <span class=\"token operator\">=</span> <span class=\"token number\">5</span> <span class=\"token comment\"># Customary to use odd and square kernel/filter size fxf </span>\nnum_filters_conv1 <span class=\"token operator\">=</span> <span class=\"token number\">32</span>\nnum_filters_conv2 <span class=\"token operator\">=</span> <span class=\"token number\">16</span>  \nC2_kernel_size <span class=\"token operator\">=</span> <span class=\"token number\">3</span> <span class=\"token comment\"># Customary to use odd and square kernel/filter size fxf </span>\nmp2d_size <span class=\"token operator\">=</span> <span class=\"token number\">2</span>      <span class=\"token comment\"># MaxPooling2d window size (= stride)</span>\nfc1_out_size <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\nfc2_out_size <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\n\nC1 <span class=\"token operator\">=</span> get_conv_size<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> C1_kernel_size<span class=\"token punctuation\">)</span>                   <span class=\"token comment\"># C1: size after conv1 </span>\nMP1 <span class=\"token operator\">=</span> get_conv_size<span class=\"token punctuation\">(</span>C1<span class=\"token punctuation\">,</span> mp2d_size<span class=\"token punctuation\">,</span> stride <span class=\"token operator\">=</span> mp2d_size<span class=\"token punctuation\">)</span> <span class=\"token comment\"># MP1: size after max_pool2d</span>\nC2 <span class=\"token operator\">=</span> get_conv_size<span class=\"token punctuation\">(</span>MP1<span class=\"token punctuation\">,</span> C2_kernel_size<span class=\"token punctuation\">)</span>                          <span class=\"token comment\"># C2: size after conv2 </span>\nMP2 <span class=\"token operator\">=</span> get_conv_size<span class=\"token punctuation\">(</span>C2<span class=\"token punctuation\">,</span> mp2d_size<span class=\"token punctuation\">,</span> stride <span class=\"token operator\">=</span> mp2d_size<span class=\"token punctuation\">)</span> <span class=\"token comment\"># MP2: size after max_pool2d</span>\nfc1_in_size <span class=\"token operator\">=</span> MP2<span class=\"token operator\">*</span>MP2 <span class=\"token operator\">*</span> num_filters_conv2                        <span class=\"token comment\"># pixels * num_filters</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'C1:</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>C1<span class=\"token punctuation\">}</span></span><span class=\"token string\"> MP1:</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>MP1<span class=\"token punctuation\">}</span></span><span class=\"token string\">   C2:</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>C2<span class=\"token punctuation\">}</span></span><span class=\"token string\"> MP2:</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>MP2<span class=\"token punctuation\">}</span></span><span class=\"token string\">    FC1_in:</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>fc1_in_size<span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">C1:24 MP1:12   C2:10 MP2:5    FC1_in:400</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">model</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>conv1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span>in_channels<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> out_channels<span class=\"token operator\">=</span>num_filters_conv1<span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span>C1_kernel_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>conv2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span>num_filters_conv1<span class=\"token punctuation\">,</span> num_filters_conv2<span class=\"token punctuation\">,</span> C2_kernel_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>fc1_in_size<span class=\"token punctuation\">,</span> fc1_out_size<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>fc1_out_size<span class=\"token punctuation\">,</span> fc2_out_size<span class=\"token punctuation\">)</span>           <span class=\"token comment\"># number of classes/digits = 10</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>F<span class=\"token punctuation\">.</span>max_pool2d<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>conv1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> mp2d_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>F<span class=\"token punctuation\">.</span>max_pool2d<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>conv2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> mp2d_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> fc1_in_size<span class=\"token punctuation\">)</span>    <span class=\"token comment\"># reshape</span>\n        x <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>dropout<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> training<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>training<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Apply dropout only during training</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fc2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> F<span class=\"token punctuation\">.</span>log_softmax<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\nnnModel <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>       <span class=\"token comment\"># Instantiate our model and move model to GPU if available</span></code></pre></div>\n<p>It should be noted, we can view the model by printing the private attribute _modules as shown below.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'nnModel parameters:\\n{}'</span><span class=\"token punctuation\">,</span>nnModel<span class=\"token punctuation\">.</span>_modules<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">nnModel parameters:\n{} OrderedDict([(&#39;conv1&#39;, Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))), (&#39;conv2&#39;, Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))), (&#39;fc1&#39;, Linear(in_features=400, out_features=100, bias=True)), (&#39;fc2&#39;, Linear(in_features=100, out_features=10, bias=True))])</code></pre></div>\n<h2>Objective or Loss Function</h2>\n<p>In the model the final layer is a log-softmax function, in orderwords we get a vector of per-digit log probabilities or log-likelihoods. The reason we have a log-softmax is because it is more numerically stable than the regular softmax. Those exponentials do tend to blow up quite easily! So the output of the model is basically</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{y} = \\log P(y | x) </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>If we combine this with the nll_loss, which stands for negative log-likelihood loss which basically does the following operation  </p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mo>−</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo stretchy=\"false\">[</mo><mi>y</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">l(\\hat{y},y) = -\\hat{y}[y]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">−</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">]</span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> is the true class label in <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><mi>C</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{0,1,...,C-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\">1</span></span></span></span></span>. If the model predicted the correct digit with probability 1.0, then this loss would be 0.0. Otherwise it would be a positive number. By combining log-softmax and NLL loss, we get the Cross Entropy Loss. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Define objective function</span>\nlossFn <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>nll_loss  <span class=\"token comment\">#When we combine nll_loss and log_softmax we get a cross entropy loss</span></code></pre></div>\n<h2>Model Optimization or Training</h2>\n<p>The optimization process is pretty simple. The new parameters are the previous parameters with a step (proportional to learning rate <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>) in the direction (negative gradient of loss function w.r.t parameters) that reduces the loss function. In terms of math for a single labeled example <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> and model prediction <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> </p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mi>n</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><mi>μ</mi><msub><mi mathvariant=\"normal\">∇</mi><mi>W</mi></msub><mi>l</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">W_n = W_{n-1} - \\mu \\nabla_{W} l(\\hat{y},y) </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.891661em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">μ</span><span class=\"mord\"><span class=\"mord\">∇</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>For a batch of examples, we simply replace the loss above with some reduction such as the mean loss over the batch. </p>\n<p>In PyTorch, we use the torch.optim module to construct an optimizer object, that will hold the current parameters and will update the parameters based on the computed gradients. To construct an Optimizer you have to give it an iterable containing the parameters to optimize accomplished by nnModel.parameters(). We can also specify optimization hyperparameters such as learning rate and momentum. </p>\n<p>All optimizers implement a step() method, that updates the parameters. At the beginning of each batch, we also need to zero out the accumulated gradients via calling zero_grad() function of the optimizer object.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Define optimization</span>\noptimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>nnModel<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span>learning_rate<span class=\"token punctuation\">,</span> momentum<span class=\"token operator\">=</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\">#optimizer = optim.Adam(nnModel.parameters(), lr=0.003)</span></code></pre></div>\n<h2>Mechanics of Training</h2>\n<p>Basically has the following steps which we repeat until all batches of training data are consumed</p>\n<ol>\n<li>Get a batch of inputs (X) and corresponding labels (y), move to device</li>\n<li>Initialize gradients</li>\n<li>Calculate loss function on current batch of inputs and labels</li>\n<li>Calculate gradients by calling backward() on the loss function output</li>\n<li>Update parameters by calling optimizer.step()</li>\n</ol>\n<h3>Some Details</h3>\n<p>The base class nn.module (so every module) has a bool property called <strong>training</strong>, which can be set to True for all modules in the model by calling the model.train() function. The flag is set to False if we call model.eval(). The idea is that modules like dropout and batchnorm can behave differently in train mode vs evaluation or prediction mode. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">train_losses <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\nTrainLen <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train_loader<span class=\"token punctuation\">.</span>dataset<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">train</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> lossFn<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> train_loader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  model<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">for</span> batch_idx<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>train_loader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># Move to device</span>\n    X_train<span class=\"token punctuation\">,</span> y_train <span class=\"token operator\">=</span> X_train<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Initialize gradients</span>\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Predict on current batch of data</span>\n    y_hat <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Calculate Average Loss</span>\n    loss <span class=\"token operator\">=</span> lossFn<span class=\"token punctuation\">(</span>y_hat<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Calculate Gradients</span>\n    loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># Update model parameters using SGC</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> batch_idx <span class=\"token operator\">%</span> log_frequency <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Train  </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>batch_idx <span class=\"token operator\">*</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">/</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>TrainLen<span class=\"token punctuation\">}</span></span><span class=\"token string\"> Loss:</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\\n'</span></span><span class=\"token punctuation\">)</span>\n      train_losses<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Performance Evaluation</h2>\n<p>How well does our trained model work? We want to evaluate the model predictions on a test set that the model has never seen. This is to see if the model generalizes and will work well in practice. In our evaluation we will track the NLL loss function over training epoch and also keep an eye on the percentage of correct predictions, called accuracy. Ideally if the test (and train) data is representive of real-world use scenarios, this should tell us how often the model is correct in the real world.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">test_losses <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">test</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> lossFn<span class=\"token punctuation\">,</span> test_loader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  model<span class=\"token punctuation\">.</span><span class=\"token builtin\">eval</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  test_loss <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n  correct <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n  <span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> X_test<span class=\"token punctuation\">,</span> y_test <span class=\"token keyword\">in</span> test_loader<span class=\"token punctuation\">:</span>\n      X_test<span class=\"token punctuation\">,</span> y_test <span class=\"token operator\">=</span> X_test<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> y_test<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>\n      y_hat <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\n      test_loss <span class=\"token operator\">+=</span> lossFn<span class=\"token punctuation\">(</span>y_hat<span class=\"token punctuation\">,</span> y_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n      pred <span class=\"token operator\">=</span> y_hat<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> keepdim<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n      correct <span class=\"token operator\">+=</span> pred<span class=\"token punctuation\">.</span>eq<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>view_as<span class=\"token punctuation\">(</span>pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  test_loss <span class=\"token operator\">/=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>test_loader<span class=\"token punctuation\">.</span>dataset<span class=\"token punctuation\">)</span>\n  test_losses<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>test_loss<span class=\"token punctuation\">)</span>\n  accuracy <span class=\"token operator\">=</span> <span class=\"token number\">100</span><span class=\"token punctuation\">.</span> <span class=\"token operator\">*</span> correct <span class=\"token operator\">/</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>test_loader<span class=\"token punctuation\">.</span>dataset<span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'\\nTest set: Avg. loss: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>test_loss<span class=\"token punctuation\">:</span><span class=\"token format-spec\">.4f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">, Accuracy: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>accuracy<span class=\"token punctuation\">}</span></span><span class=\"token string\"> %\\n'</span></span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">return</span><span class=\"token punctuation\">(</span>accuracy<span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Finally, we train!</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">test_accuracy <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\ntest_accuracy<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>test<span class=\"token punctuation\">(</span>nnModel<span class=\"token punctuation\">,</span> lossFn<span class=\"token punctuation\">,</span> test_loader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_epochs <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  train<span class=\"token punctuation\">(</span>nnModel<span class=\"token punctuation\">,</span> lossFn<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> train_loader<span class=\"token punctuation\">)</span>\n  test_accuracy<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>test<span class=\"token punctuation\">(</span>nnModel<span class=\"token punctuation\">,</span> lossFn<span class=\"token punctuation\">,</span> test_loader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Test set: Avg. loss: 0.0023, Accuracy: 8.960000038146973 %\n\nTrain  0/60000 Loss:2.3166229724884033\n\nTrain  6400/60000 Loss:1.1380994319915771\n\nTrain  12800/60000 Loss:0.6184839606285095\n\nTrain  19200/60000 Loss:0.2915685772895813\n\nTrain  25600/60000 Loss:0.39035072922706604\n\nTrain  32000/60000 Loss:0.4388900697231293\n\nTrain  38400/60000 Loss:0.3836956024169922\n\nTrain  44800/60000 Loss:0.2571248412132263\n\nTrain  51200/60000 Loss:0.1858700066804886\n\nTrain  57600/60000 Loss:0.17807245254516602\n\n\nTest set: Avg. loss: 0.0001, Accuracy: 96.36000061035156 %\n\nTrain  0/60000 Loss:0.2646735608577728\n\nTrain  6400/60000 Loss:0.25332000851631165\n\nTrain  12800/60000 Loss:0.21694515645503998\n\nTrain  19200/60000 Loss:0.1485084444284439\n\nTrain  25600/60000 Loss:0.13463640213012695\n\nTrain  32000/60000 Loss:0.08159495145082474\n\nTrain  38400/60000 Loss:0.07399110496044159\n\nTrain  44800/60000 Loss:0.2134912610054016\n\nTrain  51200/60000 Loss:0.1273864358663559\n\nTrain  57600/60000 Loss:0.328370600938797\n\n\nTest set: Avg. loss: 0.0001, Accuracy: 97.45999908447266 %\n\nTrain  0/60000 Loss:0.3802719712257385\n\nTrain  6400/60000 Loss:0.08781841397285461\n\nTrain  12800/60000 Loss:0.2607800364494324\n\nTrain  19200/60000 Loss:0.03979566693305969\n\nTrain  25600/60000 Loss:0.11034535616636276\n\nTrain  32000/60000 Loss:0.21964626014232635\n\nTrain  38400/60000 Loss:0.21965527534484863\n\nTrain  44800/60000 Loss:0.20218832790851593\n\nTrain  51200/60000 Loss:0.12613120675086975\n\nTrain  57600/60000 Loss:0.05183415114879608\n\n\nTest set: Avg. loss: 0.0001, Accuracy: 97.62000274658203 %</code></pre></div>\n<h2>Train and Accuracy Plots</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">fig<span class=\"token operator\">=</span>plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dpi<span class=\"token operator\">=</span><span class=\"token number\">60</span><span class=\"token punctuation\">)</span>\nx <span class=\"token operator\">=</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span>log_frequency<span class=\"token operator\">*</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train_losses<span class=\"token punctuation\">)</span><span class=\"token operator\">*</span>batch_size_train<span class=\"token punctuation\">,</span>log_frequency<span class=\"token operator\">*</span>batch_size_train<span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> train_losses<span class=\"token punctuation\">,</span> color<span class=\"token operator\">=</span><span class=\"token string\">'blue'</span><span class=\"token punctuation\">)</span>\nx <span class=\"token operator\">=</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> TrainLen<span class=\"token operator\">*</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>test_losses<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> TrainLen<span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>test_losses<span class=\"token punctuation\">,</span><span class=\"token string\">'r*'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>test_accuracy<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token number\">100.0</span><span class=\"token punctuation\">,</span><span class=\"token string\">'g*-'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'Train Loss'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Test Loss'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Accuracy Probability'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper right'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'number of training examples seen'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'negative log likelihood loss'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Text(0,0.5,&#39;negative log likelihood loss&#39;)</code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 511px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/58b86b2ed51d3798f350e2c24fc622d6/92e00/Pytorch_MNIST_23_1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 79.05405405405406%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAk6AAAJOgHwZJJKAAABvElEQVQ4y52Ty07cMBSGo6oLSrvi0XiIPgcItUKqxBMgpgtWPAILoEjQIrEuDCCkdjGQyzi+O77E5sSZQZMhMCOO9CfWsfPp/+04SaBGo4d1QugB52yXMTZYTnSQjujg+vpmcHd3uyeE2C+KYrvhJWXJf4R3FqU0EEziGICjCCyK7HvTqGuvQvDW++VkamO11lYpBS8dMMZ/IxBjNAEGC0Cot131LKibB0CHESgE25oAzSLgdLJ5zygCwelwsofjhQ7nQXEc+oEfjFEb8w7nHCxS16HWarNtTIE+LNzInj2cAj9i3EYWohuZVjSKa/6svh7TrG7iPzu0toqRuWgctuZIRYIw0HDmhfTLXjcyQuhb+5O2kYnG8FH17shJmqbbEchq+z8rPcJmera9Miae72yvC5SyPeWclQpT66wJtsS1EzI4TLwTwju4RRbkGAs2L7wbjz3cEu8awXx0UFXqKgLzLN1ZJpefG1vbnZdCtXf53/39V0nppdbmMMvy31LKI1Sic0HKXxTjU0zQmRD8JM2yC1Pxo/zx4Y+q5HGRF+eMkVPO6QnK0ktCyM9kifoM+gL6BFoDrYBWX1v8BF1cSrn8x2rjAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/58b86b2ed51d3798f350e2c24fc622d6/92e00/Pytorch_MNIST_23_1.png\"\n        srcset=\"/static/58b86b2ed51d3798f350e2c24fc622d6/12f09/Pytorch_MNIST_23_1.png 148w,\n/static/58b86b2ed51d3798f350e2c24fc622d6/e4a3f/Pytorch_MNIST_23_1.png 295w,\n/static/58b86b2ed51d3798f350e2c24fc622d6/92e00/Pytorch_MNIST_23_1.png 511w\"\n        sizes=\"(max-width: 511px) 100vw, 511px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Sample Test Predictions</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  output <span class=\"token operator\">=</span> nnModel<span class=\"token punctuation\">(</span>example_X<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nfig<span class=\"token operator\">=</span>plt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dpi<span class=\"token operator\">=</span><span class=\"token number\">60</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">16</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  plt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>tight_layout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>imshow<span class=\"token punctuation\">(</span>example_X<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> cmap<span class=\"token operator\">=</span><span class=\"token string\">'gray'</span><span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">\"Prediction: {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>\n    output<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> keepdim<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>xticks<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n  plt<span class=\"token punctuation\">.</span>yticks<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 547px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8e2e9ace42b96500d4df594115025420/977f7/Pytorch_MNIST_25_0.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 84.45945945945947%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAk6AAAJOgHwZJJKAAADTUlEQVQ4y12UXUiTYRTHX78/ppu4mSydsumUyaZeiEvNlUudmqgrJ1lDUOdmSrMyK/ogIaqLSgK7CCKzujG6CoxEyqSbCIIksNBRdFFIJNRNEKGc/meeSTY4vOf57fl4z/9/nldRFGUHYghxGOFQNn5axEBKSsognt6kpKRE4W2IIKI/OjrazCA2NtYmbDA9Pd2lmEymyby8PLJYLKTVamdk4e6ysjLKzc0l5IRFBoZpaWnL1dXVlJiYyPyUzL2Un59PbrebCgoKFpTk5ORJj8dDo6OjFBcX9zSyYW1tLXV0dFBnZycvNghfdjqd5HK5mJ2MbNjc3Ew9PT3M3jK4qtFoVouLi78hvyuT7PHx8auVlZWfDQbDR4z1wp9nZ2d/LS0t/YHcL2wYh682NTWtIJ9WEhISLEg6EG5oVcIzUlNT1TxG7Ec4sWm8LN6J8PB/WJclGhojLCYmpozZTYi5jrf5g/yhLKzKyMhYN5vNP7HgO8bbhb+GVr9sNhuXd0TYWVS4Bg9+I3/BYCIQCNDU1BRPeiKTdsEA6uvrY2PWMM4W/mFsbCyi4QlhF9kU1hH5GwaTLS0txJsi33SZ3WUDIMMWU4LBIFVVVW1xmec4HA5mCwpaoAhJO6IVEdYQbmuk51jHOrxBWMOoqKidomtr5BDoZhLWht4sZ3YOEUIPvcdGY3JqqU6nW4JzC2juVxhvE/4IsQg5QtD2oLAADFo2Go2LXC2De3CRuru7t5RstVppfHyc0CabJatUqlB7e/v/JV/W6/XU39+/UTI7yxra7XYGL2VSbV1dHY2MjFBNTQ3h1uQIX2loaCB+AeTnhV1HNxCqYRZi4ERZ3PXD0GCvTNKDsYtHcR0HILhK+CEYMCwOlwgrR9+OoHWOSz8qBxD3EXcQvn+adSIzM/M2nqyrWkw5I7dpAoc7xJQGlo3X4/9jzKa4Uf1+PxUWFs7LqXuKioqot7eXuByMwyWr1eovXV1d4Q+JmMm/a3zvmSNfUlDSA6/XS42NjazNrEyqqaioCOsiG0b68BN/SOrr65mdFnYlJyeHfD4fwbR3CjQZwolzWVlZz1DqBZlkRblzuI7TaJHHGOsY4oNxC3rNoC/nUfI++aR58ZiD07PY+MZfC4AA5nV8ud8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/8e2e9ace42b96500d4df594115025420/977f7/Pytorch_MNIST_25_0.png\"\n        srcset=\"/static/8e2e9ace42b96500d4df594115025420/12f09/Pytorch_MNIST_25_0.png 148w,\n/static/8e2e9ace42b96500d4df594115025420/e4a3f/Pytorch_MNIST_25_0.png 295w,\n/static/8e2e9ace42b96500d4df594115025420/977f7/Pytorch_MNIST_25_0.png 547w\"\n        sizes=\"(max-width: 547px) 100vw, 547px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>","excerpt":"In this post we will get acquainted with the basics of training a machine learning model using PyTorch. ML model development can be subdivided into the…","frontmatter":{"title":"Tutorial on recognizing digits in PyTorch","date":"2020-07-23T00:00:00.000Z","subject":["ml","tutorial","pytorch","mnist","cnn"],"author":"RSP","featimg":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAk6AAAJOgHwZJJKAAADOklEQVQ4y01U7UtTcRS+vr/Mt033ZX1oL0lrKsxNUpb6yYFu6kyEajI3t7YyEQUtaJVf+iA5TBesIoiwbQlBQoT+V718ic079fSc67nShYfzu7/n/s495zzPvYqiKG4g0tjYGKurq4tgHaqqqjIjKjU1NVcQ7gAxYKG+vv4e4g3mqqurmxACwjHuavfNzc1fsSDgnCOS8XpKubgWa2traXJyktra2jQeiXaFs+PZv3a7nVpbW0lykGKz2b6srq5SIBBQt7a2zgYHB5kIyqFoMBgkv99/iupVt9vNXEY4Gzr4E4vFCM+cGY3GcylKKZjNZorH46VcLqfKmwJyaGF8fJwrPFlfXy/l83lqb2/fZqKhocGK8BMdnCeTSXVqaqqCe1XBm39wkrm5OZLqCPO8LQkf8P3o6CjNz8/T0NAQoap3Ml+H3mY4HKa+vr6LljGHMBZvPR7PbldX1x7WO4BTEnqBNwDvvwZy/43DiLMvmbdYLHsGg4H5V0z4QTxHfAakgTXcX5VD17F+wjxmlG5qanqKPR8TULwV4SHwAuDzjyHYI+a+c6nDw8OUSqVoenqaS58Ra6SYYxXT6TQrTFIlXw52QHd3N6E7doGutFJwOp2UzWbLnZ2dFZR+KQpmucAPYX7q8fFxGVUxty0JrRDol9frpUgkUhkYGDjFHkPJIxGtra2V+CBLjzdrCeFRLeHh4eEJEZX6+/tJn5OuMnC2ubmpwg2qpjKuo5GREYIH2Yta2Ug4ywQiz4gmJiYoGo3qLb2XCq/pHwJ7UTrTWr6F/lNQKoF1Ag9EES26eTG3OOJ9QI8e4QzyWSYdDgfvLwJhroK/wy/Avslk2kf8gCQuOXQT+AR8xp4WdcFwmcRiBd6HAz6KxZQjq9VKmUyGlpeX9bJnxbxayy6XSzM2r7F32bKoTmNjYxQKhS5bzre0tFChUCglEglNFAw8IF5bkBmerKyslHiNL0tTGaOxAizK+c7Ojtrb21vRRSnCtHRwcFBeWlqqyB8lIBVqCWErFS4oS4Xaz6Gjo8OG8Nvn81GxWORzpzin2eYb/xw2NjZoZmZG/31NSxUsFGE+mtLCZcVSdoRKT08PcVJd8X9vni815kutKwAAAABJRU5ErkJggg==","aspectRatio":1.1494252873563218,"src":"/static/30f9b6ba66e77895bd0b7d2efd96bd2e/497c6/Pytorch_MNIST_5_0.png","srcSet":"/static/30f9b6ba66e77895bd0b7d2efd96bd2e/65e33/Pytorch_MNIST_5_0.png 100w,\n/static/30f9b6ba66e77895bd0b7d2efd96bd2e/69585/Pytorch_MNIST_5_0.png 200w,\n/static/30f9b6ba66e77895bd0b7d2efd96bd2e/497c6/Pytorch_MNIST_5_0.png 400w,\n/static/30f9b6ba66e77895bd0b7d2efd96bd2e/bc834/Pytorch_MNIST_5_0.png 532w","sizes":"(max-width: 400px) 100vw, 400px"}}}},"fields":{"slug":"/tutorial-pytorch-mnist/Pytorch_MNIST/","readingTime":{"text":"10 min read"}}},"site":{"siteMetadata":{"title":"JumpML"}}},"pageContext":{"slug":"/tutorial-pytorch-mnist/Pytorch_MNIST/"}}}